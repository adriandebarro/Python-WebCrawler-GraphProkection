import re
import urllib
import tokenize
from urllib.request import urlopen
import networkx  as nx


class WebCrawler:


    def __init__(self, url):
        self.StartingUrl = url
        self.toVisit = {}
        self.toVisit[url] = url
        self.UrlsToVisit = [url]
        self.Visited = {}

        
    def _getNewUrl(self):
        """
        returns a new url
        """
        currentUrl = self.UrlsToVisit[0]
        logging.info("new url added ")
        return (currentUrl, self.toVisit[currentUrl])

    def _deleteVistedUrl(self, index):
        """
        Deletes a URL from dictioary
        """
        curUrl = self.UrlsToVisit[index]
        log.info("new url added %s" %curUrl)
        urlObject = self.toVisit[curUrl]
        toVisit.pop(index)
        del self.toVisit[curUrl]
        return (curUrl,urlObject)
        
        

    def _addUrlToVisit(self, url):
        """
        
        """
        self.UrlsToVisit.append(url)
        self.toVisit[url] = url
        

    def _addVisitedUrl(self, url):
        index = self.toVisit.index(url)
        myList = deleteVistedUrl(index)
        self.VistedUrl[myList[0]] = myList[1]

    def checkIfInclded(self,url):
        reuslt = False
        for currentUrl in self.toVisit:
           if url is currentUrl:
               result = True
               break

        if self.Visited.has_key(url):
           result = True
           
        return result
        

     def urlCrawler(self):
        """
        would crawl the url and search for more urls
        """
        url = self.UrlsToVisit[0]
        self.UrlsToVisit.pop(0)
        self.Visited.append(url)
        
        while(1 > 0):
            myParsed = urlparse(url)
            print(myParsed,'\n')
            try:
                for currentUrl in re.findall('''href=["'](.[^"']+)["']''', str(urlopen(url).read()), re.I):
                    if(not(''.join(currentUrl[0:4]) == 'http') or
                       not(''.join(currentUrl[0:5]) == 'https') or
                        not(''.join(currentUrl[0:4]) == 'www')):
                        print(myParsed[0]+'//:'+myParsed[1]+str(currentUrl),'\n')

                        currentParsed = urllib.parse.urlparse(currentUrl)
                        
                        #if the domains are equal append
                        if(myParsed[1] == currentParsed[1]):
                            self.addUrlToVisit(curretUrl)
                            
            except ValueError : 
                print(str(url),'not good')
            except urllib.error.HTTPError:
                print(str(url),'not good')
            except urllib.error.URLError:
                print(str(url),'not good')
            except KeyboardInterrupt:
                print('process interrupted')
                break
            
            if(len(toVisit) > 0):
                url = toVisit[0]
                toVisit.pop(0)
                visited.append(url)
            else:
                break
            
        print('all urls visited')

    def urlRating(self, url):

        outputNumber = 0
        if not(self.StartingUrl is url):
            startingNumber = len(tokenize.generate_tokens(urllib.request.urlparse(self.StartinUrl)[3], '/'))
            currentNumber = len(tokenize.generate_tokens(urlparse(url)[3], '/'))
            outputNumber = currentNumber - startingNumber
        return outputNumber


    def crawlWeb(self):
        """
        will get new urls ad traverse them
        """
        while len(self.toVisit) > 0:
            currentUrl = self.nextUrl()
            print("next url "+currentUrl+"\n")
            self.urlCrawler(currentUrl)
            self.addVisitedUrl(currentUrl)
            

    def nextUrl(self):
        return self.UrlsToVisit[0]

    def createNode(self,currentUrl,subUrl):
        """
        will create a node for the given subUrl
        """
        self.Graph.add_node(subUrl)
        self.Graph.add_edge(currentUrl,subUrl)
        #self.toVisit[url] =  n
        #self.Graph.add_edge(currentUrl,subUrl)



        

